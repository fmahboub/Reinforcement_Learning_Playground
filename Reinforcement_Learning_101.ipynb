{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reinforcement_Learning_101.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "rzevZcgmJmhi",
        "M3KgI5g0ju3l"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fmahboub/Reinforcement_Learning_Playground/blob/main/Reinforcement_Learning_101.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Original Work\n"
      ],
      "metadata": {
        "id": "4Je3c3-sAbb6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOggIa9sU--b"
      },
      "source": [
        "## First Custom Environment\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stable_baselines3"
      ],
      "metadata": {
        "id": "dn7-9BZCBLsR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBDp4Pm-Uh4D"
      },
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "from gym import spaces\n",
        "\n",
        "\n",
        "class GoDownLeftEnv(gym.Env):\n",
        "  \"\"\"\n",
        "  Custom Environment that follows gym interface.\n",
        "  This is a simple env where the agent must learn to go always down and left. \n",
        "  \"\"\"\n",
        "  # Because of google colab, we cannot implement the GUI ('human' render mode)\n",
        "  metadata = {'render.modes': ['console']}\n",
        "  # Define constants for clearer code\n",
        "  LEFT = 0\n",
        "  RIGHT = 1\n",
        "  UP = 2\n",
        "  DOWN = 3\n",
        "\n",
        "  def __init__(self, grid_size=10):\n",
        "    super(GoDownLeftEnv, self).__init__()\n",
        "\n",
        "    # Size of the 2D-grid\n",
        "    self.grid_size = grid_size\n",
        "    # Initialize the agent at the right of the grid\n",
        "    self.agent_posx = grid_size - 1\n",
        "    self.agent_posy = grid_size - 1\n",
        "\n",
        "    # Define action and observation space\n",
        "    # They must be gym.spaces objects\n",
        "    # Example when using discrete actions, we have two: left and right\n",
        "    n_actions = 4\n",
        "    self.action_space = spaces.Discrete(n_actions)\n",
        "    # The observation will be the coordinate of the agent\n",
        "    # this can be described both by Discrete and Box space\n",
        "    self.observation_space = spaces.Box(low=1, high=self.grid_size,\n",
        "                                        shape=(2,), dtype=np.float32)\n",
        "\n",
        "  def reset(self):\n",
        "    \"\"\"\n",
        "    Important: the observation must be a numpy array\n",
        "    :return: (np.array) \n",
        "    \"\"\"\n",
        "    # Initialize the agent at the right of the grid\n",
        "    self.agent_posx = self.grid_size - 1\n",
        "    self.agent_posy = self.grid_size - 1\n",
        "    # here we convert to float32 to make it more general (in case we want to use continuous actions)\n",
        "    return np.array([self.agent_posx,self.agent_posy]).astype(np.float32)\n",
        "\n",
        "  def step(self, action):\n",
        "    if action == self.LEFT:\n",
        "      self.agent_posx -= 1\n",
        "    elif action == self.RIGHT:\n",
        "      self.agent_posx += 1\n",
        "    elif action == self.UP:\n",
        "      self.agent_posy += 1\n",
        "    elif action == self.DOWN:\n",
        "      self.agent_posy -= 1\n",
        "    else:\n",
        "      raise ValueError(\"Received invalid action={} which is not part of the action space\".format(action))\n",
        "\n",
        "    # Account for the boundaries of the grid\n",
        "    self.agent_posx = np.clip(self.agent_posx, 1, self.grid_size)\n",
        "    self.agent_posy = np.clip(self.agent_posy, 1, self.grid_size)\n",
        "\n",
        "    # Are we at the left of the grid?\n",
        "    done = bool(self.agent_posx == 1 and self.agent_posy == 1)\n",
        "\n",
        "    # Null reward everywhere except when reaching the goal (left of the grid)\n",
        "    reward = 1 if self.agent_posx == 1 and self.agent_posy == 1 else 0\n",
        "\n",
        "    # Optionally we can pass additional info, we are not using that for now\n",
        "    info = {}\n",
        "\n",
        "    return np.array([self.agent_posx, self.agent_posy]).astype(np.float32), reward, done, info\n",
        "\n",
        "  def render(self, mode='console'):\n",
        "    if mode != 'console':\n",
        "      raise NotImplementedError()\n",
        "    # agent is represented as a cross, rest as a dot\n",
        "    x_string = \". \" *(self.agent_posx-1)+\"x \"+\". \"*(self.grid_size - self.agent_posx)\n",
        "    for x in range(self.grid_size-self.agent_posy):\n",
        "      print('. '*self.grid_size)\n",
        "    print(x_string)\n",
        "    for x in range(self.agent_posy-1):\n",
        "      print('. '*self.grid_size)\n",
        "\n",
        "  def close(self):\n",
        "    pass\n",
        "    "
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nNXO_5JGTgO"
      },
      "source": [
        "env = GoDownLeftEnv()\n",
        "# If the environment don't follow the interface, an error will be thrown\n",
        "check_env(env, warn=True)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaaaDoeFGZAF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "807af406-4c49-4edf-d9d3-537c0a0f7d0a"
      },
      "source": [
        "# Train the agent\n",
        "# Instantiate the env\n",
        "env = GoDownLeftEnv(grid_size=15)\n",
        "# wrap it\n",
        "env = make_vec_env(lambda: env, n_envs=1)\n",
        "model = A2C('MlpPolicy', env, verbose=1).learn(8500)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-4cde5a559e8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# wrap it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_vec_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_envs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mA2C\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'MlpPolicy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/stable_baselines3/a2c/a2c.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0mtb_log_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtb_log_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0meval_log_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_log_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m             \u001b[0mreset_num_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset_num_timesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/stable_baselines3/common/on_policy_algorithm.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0mcontinue_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect_rollouts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrollout_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_rollout_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcontinue_training\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/stable_baselines3/common/on_policy_algorithm.py\u001b[0m in \u001b[0;36mcollect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    167\u001b[0m                 \u001b[0;31m# Convert to pytorch tensor or to TensorDict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0mobs_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobs_as_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_last_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m                 \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m             \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/stable_baselines3/common/policies.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, obs, deterministic)\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0mdistribution\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_action_dist_from_latent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent_pi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m         \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistribution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeterministic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeterministic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m         \u001b[0mlog_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistribution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    595\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_prob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/stable_baselines3/common/distributions.py\u001b[0m in \u001b[0;36mlog_prob\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/distributions/categorical.py\u001b[0m in \u001b[0;36mlog_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_pmf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/distributions/distribution.py\u001b[0m in \u001b[0;36m_validate_sample\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    284\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0msupport\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m         \u001b[0mvalid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msupport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m             raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/distributions/constraints.py\u001b[0m in \u001b[0;36mcheck\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower_bound\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupper_bound\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQ7f3pz0G0UJ"
      },
      "source": [
        "# Test the trained agent\n",
        "obs = env.reset()\n",
        "n_steps = 30\n",
        "for step in range(n_steps):\n",
        "  action, _ = model.predict(obs, deterministic=True)\n",
        "  print(\"Step {}\".format(step + 1))\n",
        "  print(\"Action: \", action)\n",
        "  obs, reward, done, info = env.step(action)\n",
        "  print('obs=', obs, 'reward=', reward, 'done=', done)\n",
        "  env.render(mode='console')\n",
        "  if done:\n",
        "    # Note that the VecEnv resets automatically\n",
        "    # when a done signal is encountered\n",
        "    print(\"Goal reached!\", \"reward=\", reward,'\\n')\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m36K511gmhPH"
      },
      "source": [
        "## Attempt 2D with Obstacles"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdWN5Nf-mpYO"
      },
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import gym\n",
        "from gym import spaces\n",
        "import random\n",
        "from stable_baselines3 import PPO, A2C # DQN coming soon\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.env_checker import check_env\n",
        "\n",
        "\n",
        "class GoDownLeftEnv(gym.Env):\n",
        "  \"\"\"\n",
        "  Custom Environment that follows gym interface.\n",
        "  This is a simple env where the agent must learn to go always left. \n",
        "  \"\"\"\n",
        "  # Because of google colab, we cannot implement the GUI ('human' render mode)\n",
        "  metadata = {'render.modes': ['console']}\n",
        "  # Define constants for clearer code\n",
        "  LEFT = 0\n",
        "  RIGHT = 1\n",
        "  UP = 2\n",
        "  DOWN = 3\n",
        "\n",
        "  def __init__(self, grid_size=10):\n",
        "    super(GoDownLeftEnv, self).__init__()\n",
        "\n",
        "    # Size of the 2D-grid\n",
        "    self.grid_size = grid_size\n",
        "    # Initialize the agent at the right of the grid\n",
        "    self.agent_posx = grid_size - 1\n",
        "    self.agent_posy = grid_size - 1\n",
        "\n",
        "    # Define action and observation space\n",
        "    # They must be gym.spaces objects\n",
        "    # Example when using discrete actions, we have two: left and right\n",
        "    n_actions = 4\n",
        "    self.action_space = spaces.Discrete(n_actions)\n",
        "    # The observation will be the coordinate of the agent\n",
        "    # this can be described both by Discrete and Box space\n",
        "    self.observation_space = spaces.Box(low=np.array([1,1,0,0,0,0]), \n",
        "                            high=np.array([self.grid_size, self.grid_size,1,1,1,1]), dtype=np.float32)\n",
        "    self.steps = 0\n",
        "\n",
        "  def reset(self):\n",
        "    \"\"\"\n",
        "    Important: the observation must be a numpy array\n",
        "    :return: (np.array) \n",
        "    \"\"\"\n",
        "    # Initialize the agent at the right of the grid\n",
        "    self.block_left = 0\n",
        "    self.block_right = 0\n",
        "    self.block_up = 0\n",
        "    self.block_down = 0\n",
        "    self.agent_posx = self.grid_size - 1\n",
        "    self.agent_posy = self.grid_size - 1\n",
        "    self.sample_size = [i+1 for i in range(self.grid_size)]\n",
        "    # random.seed(0)\n",
        "    self.env_obstacles = list(zip(random.sample(self.sample_size, int(self.grid_size/1)),\n",
        "                                  random.sample(self.sample_size, int(self.grid_size/1))))\n",
        "    try:\n",
        "      self.env_obstacles.remove((1,1))\n",
        "    except:\n",
        "      None\n",
        "    # self.env_obstacles = []\n",
        "    self.steps = 0\n",
        "\n",
        "    # here we convert to float32 to make it more general (in case we want to use continuous actions)\n",
        "    return np.array([self.agent_posx, self.agent_posy,self.block_left,self.block_right,self.block_up,\n",
        "                     self.block_down]).astype(np.float32)\n",
        "\n",
        "  def step(self, action):\n",
        "    self.steps += 1\n",
        "    self.block_left = 0\n",
        "    self.block_right = 0\n",
        "    self.block_up = 0\n",
        "    self.block_down = 0\n",
        "    if action == self.LEFT:\n",
        "      if ((self.agent_posx - 1), self.agent_posy) not in self.env_obstacles:\n",
        "        self.agent_posx -= 1\n",
        "        self.reward = 0.01\n",
        "      else:\n",
        "        self.block_left = 1\n",
        "        self.reward = 0\n",
        "    elif action == self.RIGHT:\n",
        "      if ((self.agent_posx + 1), self.agent_posy) not in self.env_obstacles:\n",
        "        self.agent_posx += 1\n",
        "        self.reward = -0.01\n",
        "      else:\n",
        "        self.block_right = 1\n",
        "        self.reward = 0\n",
        "    elif action == self.UP:\n",
        "      if ((self.agent_posx), self.agent_posy + 1) not in self.env_obstacles:\n",
        "        self.agent_posy += 1\n",
        "        self.reward = -0.01\n",
        "      else:\n",
        "        self.block_up = 1 \n",
        "        self.reward = 0\n",
        "    elif action == self.DOWN:\n",
        "      if ((self.agent_posx), self.agent_posy - 1) not in self.env_obstacles:\n",
        "        self.agent_posy -= 1\n",
        "        self.reward = 0.01\n",
        "      else:\n",
        "        self.block_down = 1\n",
        "        self.reward = 0\n",
        "    else:\n",
        "      raise ValueError(\"Received invalid action={} which is not part of the action space\".format(action))\n",
        "    self.steps += 1\n",
        "\n",
        "    # Account for the boundaries of the grid\n",
        "    if self.agent_posx < 1:\n",
        "      self.agent_posx = np.clip(self.agent_posx, 1, self.grid_size)\n",
        "      self.block_left = 1\n",
        "      self.reward = 0 \n",
        "    elif self.agent_posx > self.grid_size:\n",
        "      self.agent_posx = np.clip(self.agent_posx, 1, self.grid_size)\n",
        "      self.block_right = 1\n",
        "      self.reward = 0 \n",
        "    elif self.agent_posy < 1:\n",
        "      self.agent_posy = np.clip(self.agent_posy, 1, self.grid_size)\n",
        "      self.block_down = 1\n",
        "      self.reward = 0 \n",
        "    elif self.agent_posy > self.grid_size:\n",
        "      self.agent_posy = np.clip(self.agent_posy, 1, self.grid_size)\n",
        "      self.block_up = 1\n",
        "      self.reward = 0 \n",
        "\n",
        "    # Are we at the left of the grid?\n",
        "    done = bool(self.agent_posx == 1 and self.agent_posy == 1)\n",
        "    # done = bool(self.steps >= 30)\n",
        "\n",
        "    # Null reward everywhere except when reaching the goal (left of the grid)\n",
        "    # reward = 1 if self.agent_posx == 1 and self.agent_posy == 1 else 0\n",
        "\n",
        "    if self.agent_posx == 1 and self.agent_posy == 1:\n",
        "      min_steps = (self.grid_size-2)*2\n",
        "      reward = 1000/(self.steps-min_steps) \n",
        "      # reward = 1\n",
        "    else:\n",
        "      reward = self.reward\n",
        "      # reward = 0\n",
        "    \n",
        "    # reward = ((self.grid_size - self.agent_posx) + (self.grid_size - self.agent_posy))/(2*self.grid_size-2)\n",
        "    \n",
        "\n",
        "    # Optionally we can pass additional info, we are not using that for now\n",
        "    info = {}\n",
        "\n",
        "    return np.array([self.agent_posx, self.agent_posy,self.block_left,self.block_right,self.block_up,\n",
        "                     self.block_down]).astype(np.float32), reward, done, info\n",
        "\n",
        "  def render(self, mode='console'):\n",
        "    if mode != 'console':\n",
        "      raise NotImplementedError()\n",
        "    # agent is represented as a cross, obstacles as O, and rest as a dot\n",
        "    posx = self.agent_posx\n",
        "    posy = self.agent_posy\n",
        "    grid_size = self.grid_size\n",
        "    grid_list = []\n",
        "    # Set up grid with agent\n",
        "    for row in (range(grid_size)):\n",
        "      row_string = ''\n",
        "      for col in range(grid_size):\n",
        "        if  col+1 == posx and (grid_size-row) == posy:\n",
        "          row_string += 'x '\n",
        "        elif (col+1,(grid_size-row)) in self.env_obstacles:\n",
        "          row_string += 'O '\n",
        "        else:\n",
        "          row_string += '. '\n",
        "      grid_list.append(row_string)\n",
        "    for row in grid_list:\n",
        "      print(row)\n",
        "\n",
        "  def close(self):\n",
        "    pass\n",
        "    \n",
        "env = GoDownLeftEnv()\n",
        "# If the environment don't follow the interface, an error will be thrown\n",
        "check_env(env, warn=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNm6ihwgm2g9"
      },
      "source": [
        "# Train the agent\n",
        "# Instantiate the env\n",
        "env = GoDownLeftEnv(grid_size=20)\n",
        "# wrap it\n",
        "env = make_vec_env(lambda: env, n_envs=1)\n",
        "model = A2C('MlpPolicy', env, verbose=1, ent_coef=0.01).learn(20000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNuWKvpTm4c9"
      },
      "source": [
        "# Test the trained agent\n",
        "obs = env.reset()\n",
        "n_steps = 100\n",
        "for step in range(n_steps):\n",
        "  time.sleep(.33)\n",
        "  action, _ = model.predict(obs, deterministic=False)\n",
        "  print(\"Step {}\".format(step + 1))\n",
        "  print(\"Action: \", action)\n",
        "  obs, reward, done, info = env.step(action)\n",
        "  print('obs=', obs, 'reward=', reward, 'done=', done)\n",
        "  env.render(mode='console')\n",
        "  if done:\n",
        "    # Note that the VecEnv resets automatically\n",
        "    # when a done signal is encountered\n",
        "    if reward == 1:\n",
        "      print(\"Goal reached!\", \"reward=\", reward,'\\n')\n",
        "    if reward == 0:\n",
        "      print(\"Failure!\", \"reward=\", reward,'\\n')\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MO3HOEd7DF5"
      },
      "source": [
        "# exponential reward function (with motion reward) - 8000 iterations\n",
        "print('Exponential Reward Function')\n",
        "# exp_reward = [46.4, 34.8, 111.6, 176.0, 42.5, 191.6, 53.1, 51.1, 248.1, 259.0, 46.2, 248.3, 40.5, 194.0, 314.5, 37.4, 116.0, 54.9, 49.7, 263.8, 79.8, 50.1, 211.3, 135.9, 244.3, 41.4, 51.3, 47.2, 60.5, 88.3]\n",
        "print('Mean:',round(np.mean(exp_reward),2))\n",
        "print('Median:',np.median(exp_reward))\n",
        "print('Stdev:',np.std(exp_reward),'\\n')\n",
        "\n",
        "# Binary reward function (with motion reward)\n",
        "print('Binary Reward Function')\n",
        "# bin_reward = [252.43, 243.12, 286.23, 239.82, 37.0, 255.5, 46.18, 43.78, 55.81, 40.96, 56.81, 144.56, 218.44, 44.92, 199.27, 285.53, 192.52, 202.48, 44.96, 222.97, 106.94, 44.63, 48.72, 248.24, 208.69, 293.25, 225.53, 227.59, 46.87]\n",
        "print('Mean:',round(np.mean(bin_reward),2))\n",
        "print('Median:',np.median(bin_reward))\n",
        "print('Stdev:',np.std(bin_reward),'\\n')\n",
        "\n",
        "# Null Hypothesis (there is no difference between the average performance of these two rewards)\n",
        "from scipy import stats\n",
        "stats.ttest_ind(exp_reward, bin_reward, equal_var = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#"
      ],
      "metadata": {
        "id": "INefsNxGAH38"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sX6JOrG_wmFW"
      },
      "source": [
        "## 2D w/ Obstacles & Moving Target"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTIbZ1_PxBXQ"
      },
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import gym\n",
        "from gym import spaces\n",
        "import random\n",
        "from stable_baselines3 import PPO, A2C # DQN coming soon\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.env_checker import check_env\n",
        "\n",
        "\n",
        "class GoDownLeftEnv(gym.Env):\n",
        "  \"\"\"\n",
        "  Custom Environment that follows gym interface.\n",
        "  This is a simple env where the agent must learn to go always left. \n",
        "  \"\"\"\n",
        "  # Because of google colab, we cannot implement the GUI ('human' render mode)\n",
        "  metadata = {'render.modes': ['console']}\n",
        "  # Define constants for clearer code\n",
        "  LEFT = 0\n",
        "  RIGHT = 1\n",
        "  UP = 2\n",
        "  DOWN = 3\n",
        "\n",
        "  def __init__(self, grid_size=10):\n",
        "    super(GoDownLeftEnv, self).__init__()\n",
        "\n",
        "    # Size of the 2D-grid\n",
        "    self.grid_size = grid_size\n",
        "    # Initialize the agent at the right of the grid\n",
        "    self.agent_posx = grid_size - 1\n",
        "    self.agent_posy = grid_size - 1\n",
        "\n",
        "    # Define action and observation space\n",
        "    # They must be gym.spaces objects\n",
        "    # Example when using discrete actions, we have two: left and right\n",
        "    n_actions = 4\n",
        "    self.action_space = spaces.Discrete(n_actions)\n",
        "    # The observation will be the coordinate of the agent\n",
        "    # this can be described both by Discrete and Box space\n",
        "    self.observation_space = spaces.Box(low=np.array([0,0,0,0,0,0,0,0]), \n",
        "                  high=np.array([1,1,1,1,1,1,1,1]), dtype=np.float32)\n",
        "    self.steps = 0\n",
        "\n",
        "  def reset(self):\n",
        "    \"\"\"\n",
        "    Important: the observation must be a numpy array\n",
        "    :return: (np.array) \n",
        "    \"\"\"\n",
        "    # Initialize the agent at the right of the grid\n",
        "    self.obstacle_left = 0\n",
        "    self.obstacle_right = 0\n",
        "    self.obstacle_up = 0\n",
        "    self.obstacle_down = 0\n",
        "    self.agent_posx = self.grid_size - 1\n",
        "    self.agent_posy = self.grid_size - 1\n",
        "    self.agent_pos = (self.agent_posx, self.agent_posy)\n",
        "    self.sample_space = [i+1 for i in range(self.grid_size)]\n",
        "    # random.seed(0)\n",
        "    self.env_obstacles = list(zip(random.sample(self.sample_space, int(self.grid_size/1)),\n",
        "                                  random.sample(self.sample_space, int(self.grid_size/1))))\n",
        "    \n",
        "    # Set target and ensure it is not in the starting position of the agent\n",
        "    self.target = (random.randint(1,self.grid_size),random.randint(1,self.grid_size))\n",
        "    while self.target == self.agent_pos:\n",
        "      self.target = (random.randint(1,self.grid_size),random.randint(1,self.grid_size))\n",
        "\n",
        "    if self.target[0] < self.agent_posx: \n",
        "      self.target_dir_left = 1\n",
        "    else:\n",
        "      self.target_dir_left = 0\n",
        "    if self.target[0] > self.agent_posx: \n",
        "      self.target_dir_right = 1\n",
        "    else:\n",
        "      self.target_dir_right = 0\n",
        "    if self.target[1] > self.agent_posy:\n",
        "      self.target_dir_up = 1\n",
        "    else:\n",
        "      self.target_dir_up = 0\n",
        "    if self.target[1] < self.agent_posy:\n",
        "      self.target_dir_down = 1\n",
        "    else:\n",
        "      self.target_dir_down = 0\n",
        "    \n",
        "    # Remove any obstacles from target\n",
        "    try:\n",
        "      self.env_obstacles.remove(self.target)\n",
        "    except:\n",
        "      None\n",
        "    # self.env_obstacles = []\n",
        "    self.steps = 0\n",
        "\n",
        "    # here we convert to float32 to make it more general (in case we want to use continuous actions)\n",
        "    return np.array([self.target_dir_left, self.target_dir_right, self.target_dir_up, \n",
        "                     self.target_dir_down, self.obstacle_left,self.obstacle_right,self.obstacle_up,\n",
        "                     self.obstacle_down]).astype(np.float32)\n",
        "\n",
        "\n",
        "  def step(self, action):\n",
        "    self.steps += 1\n",
        "    self.obstacle_left = 0\n",
        "    self.obstacle_right = 0\n",
        "    self.obstacle_up = 0\n",
        "    self.obstacle_down = 0\n",
        "    if action == self.LEFT:\n",
        "      if ((self.agent_posx - 1), self.agent_posy) not in self.env_obstacles:\n",
        "        self.agent_posx -= 1\n",
        "    elif action == self.RIGHT:\n",
        "      if ((self.agent_posx + 1), self.agent_posy) not in self.env_obstacles:\n",
        "        self.agent_posx += 1\n",
        "    elif action == self.UP:\n",
        "      if ((self.agent_posx), self.agent_posy + 1) not in self.env_obstacles:\n",
        "        self.agent_posy += 1\n",
        "    elif action == self.DOWN:\n",
        "      if ((self.agent_posx), self.agent_posy - 1) not in self.env_obstacles:\n",
        "        self.agent_posy -= 1\n",
        "    else:\n",
        "      raise ValueError(\"Received invalid action={} which is not part of the action space\".format(action))\n",
        "    self.steps += 1\n",
        "\n",
        "    # Account for the boundaries of the grid\n",
        "    if self.agent_posx < 1:\n",
        "      self.agent_posx = np.clip(self.agent_posx, 1, self.grid_size)\n",
        "      self.obstacle_left = 1\n",
        "    elif self.agent_posx > self.grid_size:\n",
        "      self.agent_posx = np.clip(self.agent_posx, 1, self.grid_size)\n",
        "      self.obstacle_right = 1\n",
        "    elif self.agent_posy > self.grid_size:\n",
        "      self.agent_posy = np.clip(self.agent_posy, 1, self.grid_size)\n",
        "      self.obstacle_up = 1\n",
        "    elif self.agent_posy < 1:\n",
        "      self.agent_posy = np.clip(self.agent_posy, 1, self.grid_size)\n",
        "      self.obstacle_down = 1\n",
        "\n",
        "    # Are we at the target?\n",
        "    done = bool(self.agent_posx == self.target[0] and self.agent_posy == self.target[1])\n",
        "\n",
        "    # Null reward everywhere except when reaching the goal (left of the grid)\n",
        "    if self.agent_posx == self.target[0] and self.agent_posy == self.target[1]:\n",
        "      min_steps = np.abs(self.target[0]-self.agent_posx) + np.abs(self.target[1]-self.agent_posy)\n",
        "      reward = 1000/(self.steps-min_steps) \n",
        "    else:\n",
        "      reward = 0\n",
        "    \n",
        "    # Set target direction variables\n",
        "    if self.target[0] < self.agent_posx: \n",
        "      self.target_dir_left = 1\n",
        "    else:\n",
        "      self.target_dir_left = 0\n",
        "    if self.target[0] > self.agent_posx: \n",
        "      self.target_dir_right = 1\n",
        "    else:\n",
        "      self.target_dir_right = 0\n",
        "    if self.target[1] > self.agent_posy:\n",
        "      self.target_dir_up = 1\n",
        "    else:\n",
        "      self.target_dir_up = 0\n",
        "    if self.target[1] < self.agent_posy:\n",
        "      self.target_dir_down = 1\n",
        "    else:\n",
        "      self.target_dir_down = 0\n",
        "\n",
        "    # Set obstacle proximity variables\n",
        "    if (self.agent_posx-1,self.agent_posy) in self.env_obstacles:\n",
        "      self.obstacle_left = 1\n",
        "    if (self.agent_posx+1,self.agent_posy) in self.env_obstacles:\n",
        "      self.obstacle_right = 1\n",
        "    if (self.agent_posx,self.agent_posy+1) in self.env_obstacles:\n",
        "      self.obstacle_up = 1\n",
        "    if (self.agent_posx,self.agent_posy-1) in self.env_obstacles:\n",
        "      self.obstacle_down = 1\n",
        "    \n",
        "    # Optionally we can pass additional info, we are not using that for now\n",
        "    info = {}\n",
        "\n",
        "    return np.array([self.target_dir_left, self.target_dir_right, self.target_dir_up, \n",
        "                     self.target_dir_down, self.obstacle_left,self.obstacle_right,self.obstacle_up,\n",
        "                     self.obstacle_down]).astype(np.float32), reward, done, info\n",
        "\n",
        "  def render(self, mode='console'):\n",
        "    if mode != 'console':\n",
        "      raise NotImplementedError()\n",
        "    # agent is represented as a cross, obstacles as O, and rest as a dot\n",
        "    posx = self.agent_posx\n",
        "    posy = self.agent_posy\n",
        "    grid_size = self.grid_size\n",
        "    grid_list = []\n",
        "    # Set up grid with agent\n",
        "    for row in (range(grid_size)):\n",
        "      row_string = ''\n",
        "      for col in range(grid_size):\n",
        "        if  col+1 == posx and (grid_size-row) == posy:\n",
        "          row_string += 'A '\n",
        "        elif (col+1,(grid_size-row)) in self.env_obstacles:\n",
        "          row_string += 'O '\n",
        "        elif (col+1,(grid_size-row)) == self.target:\n",
        "          row_string += 'X '\n",
        "        else:\n",
        "          row_string += '. '\n",
        "      grid_list.append(row_string)\n",
        "    for row in grid_list:\n",
        "      print(row)\n",
        "\n",
        "  def close(self):\n",
        "    pass\n",
        "    \n",
        "env = GoDownLeftEnv()\n",
        "# If the environment don't follow the interface, an error will be thrown\n",
        "check_env(env, warn=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCpKQKhmypw6"
      },
      "source": [
        "# Train the agent\n",
        "# Instantiate the env\n",
        "start = time.time()\n",
        "env = GoDownLeftEnv(grid_size=25)\n",
        "# wrap it\n",
        "env = make_vec_env(lambda: env, n_envs=1)\n",
        "model = A2C('MlpPolicy', env, verbose=1, ent_coef=0.01).learn(50000)\n",
        "print(\"Time Taken:\",round(time.time()-start,2),'s')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHYKZvVuy6U5"
      },
      "source": [
        "# Test the trained agent\n",
        "for x in range(5):\n",
        "  obs = env.reset()\n",
        "  n_steps = 100\n",
        "  for step in range(n_steps):\n",
        "    time.sleep(.29)\n",
        "    action, _ = model.predict(obs, deterministic=False)\n",
        "    print(\"Step {}\".format(step + 1))\n",
        "    print(\"Action: \", action)\n",
        "    obs, reward, done, info = env.step(action)\n",
        "    print('obs=', obs, 'reward=', reward, 'done=', done)\n",
        "    env.render(mode='console')\n",
        "    if done:\n",
        "      # Note that the VecEnv resets automatically\n",
        "      # when a done signal is encountered\n",
        "      if reward == 1:\n",
        "        print(\"Goal reached!\", \"reward=\", reward,'\\n')\n",
        "      if reward == 0:\n",
        "        print(\"Failure!\", \"reward=\", reward,'\\n')\n",
        "      break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3K7P1iVJvlcb"
      },
      "source": [
        "## PyGame + Gym"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stable_baselines3"
      ],
      "metadata": {
        "id": "Xh7vPnA_3nYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRBIQeFxq8cb"
      },
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import gym\n",
        "from gym import spaces\n",
        "import random\n",
        "from stable_baselines3 import PPO, A2C # DQN coming soon\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.env_checker import check_env\n",
        "import os\n",
        "import sys\n",
        "import random\n",
        "import time\n",
        "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\n",
        "import pygame\n",
        "from google.colab.patches import cv2_imshow\n",
        "from google.colab import output\n",
        "\n",
        "\n",
        "class GoDownLeftEnv(gym.Env):\n",
        "  \"\"\"\n",
        "  Custom Environment that follows gym interface.\n",
        "  This is a simple env where the agent must learn to go always left. \n",
        "  \"\"\"\n",
        "  # Because of google colab, we cannot implement the GUI ('human' render mode)\n",
        "  metadata = {'render.modes': ['console']}\n",
        "  # Define constants for clearer code\n",
        "  LEFT = 0\n",
        "  RIGHT = 1\n",
        "  UP = 2\n",
        "  DOWN = 3\n",
        "\n",
        "  def __init__(self, grid_size=10):\n",
        "    super(GoDownLeftEnv, self).__init__()\n",
        "\n",
        "    # Size of the 2D-grid\n",
        "    self.grid_size = grid_size\n",
        "    # Initialize the agent at the right of the grid\n",
        "    self.agent_posx = grid_size - 1\n",
        "    self.agent_posy = grid_size - 1\n",
        "    self.colour = (17, 24, 47)\n",
        "\n",
        "    # Define action and observation space\n",
        "    # They must be gym.spaces objects\n",
        "    # Example when using discrete actions, we have two: left and right\n",
        "    n_actions = 4\n",
        "    self.action_space = spaces.Discrete(n_actions)\n",
        "    # The observation will be the coordinate of the agent\n",
        "    # this can be described both by Discrete and Box space\n",
        "    self.observation_space = spaces.Box(low=np.array([0,0,0,0,0,0,0,0]), \n",
        "                  high=np.array([1,1,1,1,1,1,1,1]), dtype=np.float32)\n",
        "    self.steps = 0\n",
        "\n",
        "  def reset(self):\n",
        "    \"\"\"\n",
        "    Important: the observation must be a numpy array\n",
        "    :return: (np.array) \n",
        "    \"\"\"\n",
        "    # Initialize the agent at the right of the grid\n",
        "    self.obstacle_left = 0\n",
        "    self.obstacle_right = 0\n",
        "    self.obstacle_up = 0\n",
        "    self.obstacle_down = 0\n",
        "    self.agent_posx = self.grid_size - 1\n",
        "    self.agent_posy = self.grid_size - 1\n",
        "    self.agent_pos = (self.agent_posx, self.agent_posy)\n",
        "    self.sample_space = [i+1 for i in range(self.grid_size)]\n",
        "    # random.seed(0)\n",
        "    self.env_obstacles = list(zip(random.sample(self.sample_space, int(self.grid_size/1)),\n",
        "                                  random.sample(self.sample_space, int(self.grid_size/1))))\n",
        "    \n",
        "    # Set target and ensure it is not in the starting position of the agent\n",
        "    self.target = (random.randint(1,self.grid_size),random.randint(1,self.grid_size))\n",
        "    while self.target == self.agent_pos:\n",
        "      self.target = (random.randint(1,self.grid_size),random.randint(1,self.grid_size))\n",
        "\n",
        "    if self.target[0] < self.agent_posx: \n",
        "      self.target_dir_left = 1\n",
        "    else:\n",
        "      self.target_dir_left = 0\n",
        "    if self.target[0] > self.agent_posx: \n",
        "      self.target_dir_right = 1\n",
        "    else:\n",
        "      self.target_dir_right = 0\n",
        "    if self.target[1] > self.agent_posy:\n",
        "      self.target_dir_up = 1\n",
        "    else:\n",
        "      self.target_dir_up = 0\n",
        "    if self.target[1] < self.agent_posy:\n",
        "      self.target_dir_down = 1\n",
        "    else:\n",
        "      self.target_dir_down = 0\n",
        "\n",
        "    # PYGAME SECTION ----------------------------\n",
        "    self.screen_width = self.grid_size*self.grid_size\n",
        "    self.screen_height = self.grid_size*self.grid_size\n",
        "    self.grid_width = self.screen_width/self.grid_size\n",
        "    self.grid_height = self.screen_height/self.grid_size\n",
        "    pygame.init()\n",
        "    clock = pygame.time.Clock()\n",
        "    self.screen = pygame.display.set_mode((self.screen_width, self.screen_height), 0, 32)\n",
        "\n",
        "    self.myfont = pygame.font.SysFont(\"monospace\",16)\n",
        "    self.surface = pygame.Surface(self.screen.get_size())\n",
        "    self.surface = self.surface.convert()\n",
        "    # drawGrid(surface)\n",
        "    #  -------------------------------------------\n",
        "    \n",
        "    # Remove any obstacles from target\n",
        "    try:\n",
        "      self.env_obstacles.remove(self.target)\n",
        "    except:\n",
        "      None\n",
        "    # self.env_obstacles = []\n",
        "    self.steps = 0\n",
        "\n",
        "    # here we convert to float32 to make it more general (in case we want to use continuous actions)\n",
        "    return np.array([self.target_dir_left, self.target_dir_right, self.target_dir_up, \n",
        "                     self.target_dir_down, self.obstacle_left,self.obstacle_right,self.obstacle_up,\n",
        "                     self.obstacle_down]).astype(np.float32)\n",
        "\n",
        "\n",
        "  def step(self, action):\n",
        "    self.steps += 1\n",
        "    self.obstacle_left = 0\n",
        "    self.obstacle_right = 0\n",
        "    self.obstacle_up = 0\n",
        "    self.obstacle_down = 0\n",
        "    if action == self.LEFT:\n",
        "      if ((self.agent_posx - 1), self.agent_posy) not in self.env_obstacles:\n",
        "        self.agent_posx -= 1\n",
        "    elif action == self.RIGHT:\n",
        "      if ((self.agent_posx + 1), self.agent_posy) not in self.env_obstacles:\n",
        "        self.agent_posx += 1\n",
        "    elif action == self.UP:\n",
        "      if ((self.agent_posx), self.agent_posy + 1) not in self.env_obstacles:\n",
        "        self.agent_posy += 1\n",
        "    elif action == self.DOWN:\n",
        "      if ((self.agent_posx), self.agent_posy - 1) not in self.env_obstacles:\n",
        "        self.agent_posy -= 1\n",
        "    else:\n",
        "      raise ValueError(\"Received invalid action={} which is not part of the action space\".format(action))\n",
        "    self.steps += 1\n",
        "\n",
        "    # Account for the boundaries of the grid\n",
        "    if self.agent_posx < 1:\n",
        "      self.agent_posx = np.clip(self.agent_posx, 1, self.grid_size)\n",
        "      self.obstacle_left = 1\n",
        "    elif self.agent_posx > self.grid_size:\n",
        "      self.agent_posx = np.clip(self.agent_posx, 1, self.grid_size)\n",
        "      self.obstacle_right = 1\n",
        "    elif self.agent_posy > self.grid_size:\n",
        "      self.agent_posy = np.clip(self.agent_posy, 1, self.grid_size)\n",
        "      self.obstacle_up = 1\n",
        "    elif self.agent_posy < 1:\n",
        "      self.agent_posy = np.clip(self.agent_posy, 1, self.grid_size)\n",
        "      self.obstacle_down = 1\n",
        "\n",
        "    # Are we at the target?\n",
        "    done = bool(self.agent_posx == self.target[0] and self.agent_posy == self.target[1])\n",
        "\n",
        "    # Null reward everywhere except when reaching the goal (left of the grid)\n",
        "    if self.agent_posx == self.target[0] and self.agent_posy == self.target[1]:\n",
        "      min_steps = np.abs(self.target[0]-self.agent_posx) + np.abs(self.target[1]-self.agent_posy)\n",
        "      reward = 1000/(self.steps-min_steps) \n",
        "    else:\n",
        "      reward = 0\n",
        "    self.reward = reward\n",
        "\n",
        "    # Set target direction variables\n",
        "    if self.target[0] < self.agent_posx: \n",
        "      self.target_dir_left = 1\n",
        "    else:\n",
        "      self.target_dir_left = 0\n",
        "    if self.target[0] > self.agent_posx: \n",
        "      self.target_dir_right = 1\n",
        "    else:\n",
        "      self.target_dir_right = 0\n",
        "    if self.target[1] > self.agent_posy:\n",
        "      self.target_dir_up = 1\n",
        "    else:\n",
        "      self.target_dir_up = 0\n",
        "    if self.target[1] < self.agent_posy:\n",
        "      self.target_dir_down = 1\n",
        "    else:\n",
        "      self.target_dir_down = 0\n",
        "\n",
        "    # Set obstacle proximity variables\n",
        "    if (self.agent_posx-1,self.agent_posy) in self.env_obstacles:\n",
        "      self.obstacle_left = 1\n",
        "    if (self.agent_posx+1,self.agent_posy) in self.env_obstacles:\n",
        "      self.obstacle_right = 1\n",
        "    if (self.agent_posx,self.agent_posy+1) in self.env_obstacles:\n",
        "      self.obstacle_up = 1\n",
        "    if (self.agent_posx,self.agent_posy-1) in self.env_obstacles:\n",
        "      self.obstacle_down = 1\n",
        "    \n",
        "    # Optionally we can pass additional info, we are not using that for now\n",
        "    info = {'PosX':self.agent_posx,'PosY':self.agent_posy}\n",
        "\n",
        "    return np.array([self.target_dir_left, self.target_dir_right, self.target_dir_up, \n",
        "                     self.target_dir_down, self.obstacle_left,self.obstacle_right,self.obstacle_up,\n",
        "                     self.obstacle_down]).astype(np.float32), reward, done, info\n",
        "\n",
        "  def render(self, mode='console'):\n",
        "    if mode != 'console':\n",
        "      raise NotImplementedError()\n",
        "\n",
        "    # PYGAME SECTION ----------------------------\n",
        "    # drawGrid(surface)\n",
        "    for y in range(0, int(self.grid_height)):\n",
        "      for x in range(0, int(self.grid_width)):\n",
        "          if (x+y)%2 == 0:\n",
        "              r = pygame.Rect((x*self.grid_size, y*self.grid_size), (self.grid_size,self.grid_size))\n",
        "              pygame.draw.rect(self.surface,(93,216,228), r)\n",
        "          else:\n",
        "              rr = pygame.Rect((x*self.grid_size, y*self.grid_size), (self.grid_size,self.grid_size))\n",
        "              pygame.draw.rect(self.surface, (84,194,205), rr)\n",
        "    # Draw Agent\n",
        "    r = pygame.Rect(((self.agent_posx-1)*self.grid_size, (self.agent_posy*-1 + self.grid_size)*self.grid_size), (self.grid_size,self.grid_size))\n",
        "    pygame.draw.rect(self.surface, (0,0,255), r, 0)\n",
        "    # Draw Obstacles\n",
        "    for ob in self.env_obstacles:\n",
        "        r = pygame.Rect(((ob[0]-1)*self.grid_size, (ob[1]*-1+self.grid_size)*self.grid_size), (self.grid_size,self.grid_size))\n",
        "        pygame.draw.rect(self.surface, (0,0,0), r, 0)\n",
        "    # Draw Target\n",
        "    r = pygame.Rect(((self.target[0]-1)*self.grid_size, (self.target[1]*-1 + self.grid_size)*self.grid_size), (self.grid_size,self.grid_size))\n",
        "    pygame.draw.rect(self.surface, (255,255,255), r, 0)\n",
        "\n",
        "    self.screen.blit(self.surface, (0,0))\n",
        "    # text = self.myfont.render(\"Reward {0}\".format(self.reward), 1, (0,0,0))\n",
        "    # self.screen.blit(text, (5,10))\n",
        "    pygame.display.update()\n",
        "    image = pygame.surfarray.array3d(self.screen)\n",
        "    image = image.transpose([1, 0, 2])\n",
        "    # print('\\n')\n",
        "    cv2_imshow(image)\n",
        "  def close(self):\n",
        "    pass\n",
        "    \n",
        "env = GoDownLeftEnv()\n",
        "# If the environment don't follow the interface, an error will be thrown\n",
        "check_env(env, warn=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzHFQv5GLi2n"
      },
      "source": [
        "# grid_size = 25\n",
        "# agent_posx = grid_size*1\n",
        "# agent_posy = grid_size*1\n",
        "# screen_width = grid_size*25\n",
        "# screen_height = grid_size*25\n",
        "# colour = (17, 24, 47)\n",
        "# grid_width = screen_width/grid_size\n",
        "# grid_height = screen_height/grid_size\n",
        "# sample_space = [i+1 for i in range(grid_size)]\n",
        "# random.seed(0)\n",
        "# env_obstacles = list(zip(random.sample(sample_space, int(grid_size/1)),\n",
        "#                                   random.sample(sample_space, int(grid_size/1))))\n",
        "# pygame.init()\n",
        "# screen = pygame.display.set_mode((screen_width, screen_height), 0, 32)\n",
        "# surface = pygame.Surface(screen.get_size())\n",
        "# surface = surface.convert()\n",
        "# myfont = pygame.font.SysFont(\"monospace\",16)\n",
        "# # Draw Background\n",
        "# for y in range(0, int(grid_height)):\n",
        "#   for x in range(0, int(grid_width)):\n",
        "#       if (x+y)%2 == 0:\n",
        "#           r = pygame.Rect((x*grid_size, y*grid_size), (grid_size,grid_size))\n",
        "#           pygame.draw.rect(surface,(93,216,228), r)\n",
        "#       else:\n",
        "#           rr = pygame.Rect((x*grid_size, y*grid_size), (grid_size,grid_size))\n",
        "#           pygame.draw.rect(surface, (84,194,205), rr)\n",
        "# # Draw Agent\n",
        "# r = pygame.Rect((agent_posx, agent_posy), (grid_size,grid_size))\n",
        "# pygame.draw.rect(surface, (0,0,255), r,0)\n",
        "# # Draw Obstacles\n",
        "# for ob in env_obstacles:\n",
        "#     # print(ob)\n",
        "#     r = pygame.Rect(((ob[0]-1)*grid_size, (ob[1]*-1+grid_size)*grid_size), (grid_size,grid_size))\n",
        "#     pygame.draw.rect(surface, (0,0, 0), r, 0)\n",
        "\n",
        "# screen.blit(surface, (0,0))\n",
        "# text = myfont.render(\"Reward {0}\".format(0), 1, (0,0,0))\n",
        "# screen.blit(text, (5,10))\n",
        "# pygame.display.update()\n",
        "# image = pygame.surfarray.array3d(screen)\n",
        "# image = image.transpose([1, 0, 2])\n",
        "# print('\\n')\n",
        "# cv2_imshow(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HoXtSy8-2nb1"
      },
      "source": [
        "# Train the agent\n",
        "# Instantiate the env\n",
        "start = time.time()\n",
        "env = GoDownLeftEnv(grid_size=20)\n",
        "# wrap it\n",
        "env = make_vec_env(lambda: env, n_envs=1)\n",
        "model = A2C('MlpPolicy', env, verbose=1, ent_coef=0.01).learn(25000)\n",
        "print(\"Time Taken:\",round(time.time()-start,2),'s')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlI1O_Km5BVr"
      },
      "source": [
        "# Test the trained agent\n",
        "\n",
        "for x in range(5):\n",
        "  obs = env.reset()\n",
        "  n_steps = 40\n",
        "  for step in range(n_steps):\n",
        "    time.sleep(.25)\n",
        "    action, _ = model.predict(obs, deterministic=False)\n",
        "    print('\\n'+\"Step {}\".format(step + 1))\n",
        "    print(\"Action: \", action)\n",
        "    obs, reward, done, info = env.step(action)\n",
        "    print('obs=', obs, 'reward=', reward, 'done=', done, info)\n",
        "    env.render(mode='console')\n",
        "    if done:\n",
        "      # Note that the VecEnv resets automatically\n",
        "      # when a done signal is encountered\n",
        "      if reward == 1:\n",
        "        print(\"Goal reached!\", \"reward=\", reward,'\\n')\n",
        "      if reward == 0:\n",
        "        print(\"Failure!\", \"reward=\", reward,'\\n')\n",
        "      break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TUTORIALS**"
      ],
      "metadata": {
        "id": "v6BMOCqgA8vx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7zFGrfABGJR"
      },
      "source": [
        "# PyGame - Random Snake Tutorial\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvZWKPYFBGJR"
      },
      "source": [
        "!pip install pygame"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MZd8WrFBGJR"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import random\n",
        "import time\n",
        "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\n",
        "import pygame\n",
        "from google.colab.patches import cv2_imshow\n",
        "from google.colab import output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5LnWQ8sBGJS"
      },
      "source": [
        "class Snake():\n",
        "    def __init__(self):\n",
        "        self.length = 1\n",
        "        self.positions = [((screen_width/2), (screen_height/2))]\n",
        "        self.direction = random.choice([up, down, left, right])\n",
        "        self.color = (17, 24, 47)\n",
        "        # Special thanks to YouTubers Mini - Cafetos and Knivens Beast for raising this issue!\n",
        "        # Code adjustment courtesy of YouTuber Elija de Hoog\n",
        "        self.score = 0\n",
        "\n",
        "    def get_head_position(self):\n",
        "        return self.positions[0]\n",
        "\n",
        "    def turn(self, point):\n",
        "        if self.length > 1 and (point[0]*-1, point[1]*-1) == self.direction:\n",
        "            return\n",
        "        else:\n",
        "            self.direction = point\n",
        "\n",
        "    def move(self):\n",
        "        cur = self.get_head_position()\n",
        "        x,y = self.direction\n",
        "        new = (((cur[0]+(x*gridsize))%screen_width), (cur[1]+(y*gridsize))%screen_height)\n",
        "        if len(self.positions) > 2 and new in self.positions[2:]:\n",
        "            self.reset()\n",
        "        else:\n",
        "            self.positions.insert(0,new)\n",
        "            if len(self.positions) > self.length:\n",
        "                self.positions.pop()\n",
        "\n",
        "    def reset(self):\n",
        "        self.length = 1\n",
        "        self.positions = [((screen_width/2), (screen_height/2))]\n",
        "        self.direction = random.choice([up, down, left, right])\n",
        "        self.score = 0\n",
        "\n",
        "    def draw(self,surface):\n",
        "        for p in self.positions:\n",
        "            r = pygame.Rect((p[0], p[1]), (gridsize,gridsize))\n",
        "            pygame.draw.rect(surface, self.color, r)\n",
        "            pygame.draw.rect(surface, (93,216, 228), r, 1)\n",
        "\n",
        "    def handle_keys(self):\n",
        "      var = random.uniform(0,1)\n",
        "      if var >= 0.75:\n",
        "          self.turn(up)\n",
        "      elif var >= 0.5:\n",
        "          self.turn(down)\n",
        "      elif var >= 0.25:\n",
        "          self.turn(left)\n",
        "      else:\n",
        "          self.turn(right)\n",
        "\n",
        "class Food():\n",
        "    def __init__(self):\n",
        "        self.position = (0,0)\n",
        "        self.color = (223, 163, 49)\n",
        "        self.randomize_position()\n",
        "\n",
        "    def randomize_position(self):\n",
        "        self.position = (random.randint(0, grid_width-1)*gridsize, random.randint(0, grid_height-1)*gridsize)\n",
        "\n",
        "    def draw(self, surface):\n",
        "        r = pygame.Rect((self.position[0], self.position[1]), (gridsize, gridsize))\n",
        "        pygame.draw.rect(surface, self.color, r)\n",
        "        pygame.draw.rect(surface, (93, 216, 228), r, 1)\n",
        "\n",
        "def drawGrid(surface):\n",
        "    for y in range(0, int(grid_height)):\n",
        "        for x in range(0, int(grid_width)):\n",
        "            if (x+y)%2 == 0:\n",
        "                r = pygame.Rect((x*gridsize, y*gridsize), (gridsize,gridsize))\n",
        "                pygame.draw.rect(surface,(93,216,228), r)\n",
        "            else:\n",
        "                rr = pygame.Rect((x*gridsize, y*gridsize), (gridsize,gridsize))\n",
        "                pygame.draw.rect(surface, (84,194,205), rr)\n",
        "\n",
        "screen_width = 480\n",
        "screen_height = 480\n",
        "\n",
        "gridsize = 20\n",
        "grid_width = screen_width/gridsize\n",
        "grid_height = screen_height/gridsize\n",
        "\n",
        "up = (0,-1)\n",
        "down = (0,1)\n",
        "left = (-1,0)\n",
        "right = (1,0)\n",
        "\n",
        "def main():\n",
        "    pygame.init()\n",
        "\n",
        "    clock = pygame.time.Clock()\n",
        "    screen = pygame.display.set_mode((screen_width, screen_height), 0, 32)\n",
        "\n",
        "    surface = pygame.Surface(screen.get_size())\n",
        "    surface = surface.convert()\n",
        "    drawGrid(surface)\n",
        "\n",
        "    snake = Snake()\n",
        "    food = Food()\n",
        "\n",
        "    myfont = pygame.font.SysFont(\"monospace\",16)\n",
        "\n",
        "    while (True):\n",
        "        clock.tick(4)\n",
        "        snake.handle_keys()\n",
        "        drawGrid(surface)\n",
        "        snake.move()\n",
        "        if snake.get_head_position() == food.position:\n",
        "            snake.length += 1\n",
        "            snake.score += 1\n",
        "            food.randomize_position()\n",
        "        snake.draw(surface)\n",
        "        food.draw(surface)\n",
        "        screen.blit(surface, (0,0))\n",
        "        text = myfont.render(\"Score {0}\".format(snake.score), 1, (0,0,0))\n",
        "        screen.blit(text, (5,10))\n",
        "        pygame.display.update()\n",
        "        image = pygame.surfarray.array3d(screen)\n",
        "        image = image.transpose([1, 0, 2])\n",
        "        # output.clear(wait=True)\n",
        "        print('\\n')\n",
        "        cv2_imshow(image)\n",
        "\n",
        "main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AoxOjIlOImwx"
      },
      "source": [
        "# Stable Baselines3 Tutorial\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a custom Gym environment\n",
        "\n",
        "Github repo: https://github.com/araffin/rl-tutorial-jnrr19/tree/sb3/\n",
        "\n",
        "Stable-Baselines3: https://github.com/DLR-RM/stable-baselines3\n",
        "\n",
        "Documentation: https://stable-baselines3.readthedocs.io/en/master/\n",
        "\n",
        "RL Baselines3 zoo: https://github.com/DLR-RM/rl-baselines3-zoo\n",
        "\n",
        "\n",
        "## Introduction\n",
        "\n",
        "In this notebook, you will learn how to use your own environment following the OpenAI Gym interface.\n",
        "Once it is done, you can easily use any compatible (depending on the action space) RL algorithm from Stable Baselines on that environment.\n",
        "\n",
        "## Install Dependencies and Stable Baselines3 Using Pip\n"
      ],
      "metadata": {
        "id": "tn2yWG4tBd11"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sp8rSS4DIhEV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e5b4661-827c-427c-e619-c28f69a0e7b0"
      },
      "source": [
        "!pip install stable-baselines3[extra]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: stable-baselines3[extra] in /usr/local/lib/python3.7/dist-packages (1.4.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (1.1.5)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (1.3.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (3.2.2)\n",
            "Requirement already satisfied: gym<0.20,>=0.17 in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (0.17.3)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (1.10.0+cu111)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (1.19.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (5.4.8)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (4.1.2.30)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (7.1.2)\n",
            "Collecting atari-py==0.2.6\n",
            "  Downloading atari_py-0.2.6-cp37-cp37m-manylinux1_x86_64.whl (2.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.8 MB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (2.7.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from atari-py==0.2.6->stable-baselines3[extra]) (1.15.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym<0.20,>=0.17->stable-baselines3[extra]) (1.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym<0.20,>=0.17->stable-baselines3[extra]) (1.4.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym<0.20,>=0.17->stable-baselines3[extra]) (0.16.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (1.35.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (3.3.6)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (1.43.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (2.23.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (1.0.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (1.8.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (0.4.6)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (3.17.3)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (0.37.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (57.4.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->stable-baselines3[extra]) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->stable-baselines3[extra]) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->stable-baselines3[extra]) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->stable-baselines3[extra]) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->stable-baselines3[extra]) (4.10.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->stable-baselines3[extra]) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->stable-baselines3[extra]) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->stable-baselines3[extra]) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->stable-baselines3[extra]) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->stable-baselines3[extra]) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->stable-baselines3[extra]) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->stable-baselines3[extra]) (2021.10.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->stable-baselines3[extra]) (3.1.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[extra]) (3.0.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[extra]) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[extra]) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[extra]) (0.11.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->stable-baselines3[extra]) (2018.9)\n",
            "\u001b[33mWARNING: The candidate selected for download or install is a yanked version: 'atari-py' candidate (version 0.2.6 at https://files.pythonhosted.org/packages/8f/ba/1d22e9d2f332f07aaa57041f5dd569c2cb40a92bd6374a0b743ec3dfae97/atari_py-0.2.6-cp37-cp37m-manylinux1_x86_64.whl#sha256=d9e2c25d39783867c2f29d1dd9d3a659fc56036456d07dc9efe8bd7bb31a07d7 (from https://pypi.org/simple/atari-py/))\n",
            "Reason for being yanked: re-release with new wheels\u001b[0m\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3021, in _dep_map\n",
            "    return self.__dep_map\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2815, in __getattr__\n",
            "    raise AttributeError(attr)\n",
            "AttributeError: _DistInfoDistribution__dep_map\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/base_command.py\", line 180, in _main\n",
            "    status = self.run(options, args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/req_command.py\", line 199, in wrapper\n",
            "    return func(self, options, args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/commands/install.py\", line 385, in run\n",
            "    conflicts = self._determine_conflicts(to_install)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/commands/install.py\", line 515, in _determine_conflicts\n",
            "    return check_install_conflicts(to_install)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/operations/check.py\", line 103, in check_install_conflicts\n",
            "    package_set, _ = create_package_set_from_installed()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/operations/check.py\", line 45, in create_package_set_from_installed\n",
            "    package_set[name] = PackageDetails(dist.version, dist.requires())\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2736, in requires\n",
            "    dm = self._dep_map\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3023, in _dep_map\n",
            "    self.__dep_map = self._compute_dependencies()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3033, in _compute_dependencies\n",
            "    reqs.extend(parse_requirements(req))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3094, in parse_requirements\n",
            "    yield Requirement(line)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3101, in __init__\n",
            "    super(Requirement, self).__init__(requirement_string)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/packaging/requirements.py\", line 113, in __init__\n",
            "    req = REQUIREMENT.parseString(requirement_string)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 1943, in parseString\n",
            "    loc, tokens = self._parse(instring, 0)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 1683, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, preloc, doActions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 4069, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 1683, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, preloc, doActions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 4254, in parseImpl\n",
            "    ret = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 1683, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, preloc, doActions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 4069, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 1683, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, preloc, doActions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 4849, in parseImpl\n",
            "    loc, tokens = self.expr._parse(instring, loc, doActions, callPreParse=False)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 1683, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, preloc, doActions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 4069, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 1716, in _parseNoCache\n",
            "    tokens = fn(instring, tokensStart, retTokens)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 1316, in wrapper\n",
            "    ret = func(*args[limit[0]:])\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/packaging/requirements.py\", line 81, in <lambda>\n",
            "    lambda s, l, t: Marker(s[t._original_start : t._original_end])\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/packaging/markers.py\", line 307, in __init__\n",
            "    self._markers = _coerce_parse_result(MARKER.parseString(marker))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 1943, in parseString\n",
            "    loc, tokens = self._parse(instring, 0)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 1683, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, preloc, doActions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 4069, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 1683, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, preloc, doActions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 4462, in parseImpl\n",
            "    return self.expr._parse(instring, loc, doActions, callPreParse=False)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 1683, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, preloc, doActions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 4069, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 1683, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, preloc, doActions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 4781, in parseImpl\n",
            "    return super(ZeroOrMore, self).parseImpl(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 4697, in parseImpl\n",
            "    loc, tokens = self_expr_parse(instring, loc, doActions, callPreParse=False)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 1683, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, preloc, doActions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 4052, in parseImpl\n",
            "    loc, resultlist = self.exprs[0]._parse(instring, loc, doActions, callPreParse=False)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/main.py\", line 71, in main\n",
            "    return command.main(cmd_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/base_command.py\", line 104, in main\n",
            "    return self._main(args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/base_command.py\", line 212, in _main\n",
            "    logger.critical(\"Operation cancelled by user\")\n",
            "  File \"/usr/lib/python3.7/logging/__init__.py\", line 1425, in critical\n",
            "    self._log(CRITICAL, msg, args, **kwargs)\n",
            "  File \"/usr/lib/python3.7/logging/__init__.py\", line 1514, in _log\n",
            "    self.handle(record)\n",
            "  File \"/usr/lib/python3.7/logging/__init__.py\", line 1524, in handle\n",
            "    self.callHandlers(record)\n",
            "  File \"/usr/lib/python3.7/logging/__init__.py\", line 1586, in callHandlers\n",
            "    hdlr.handle(record)\n",
            "  File \"/usr/lib/python3.7/logging/__init__.py\", line 894, in handle\n",
            "    self.emit(record)\n",
            "  File \"/usr/lib/python3.7/logging/__init__.py\", line 1028, in emit\n",
            "    stream.write(msg + self.terminator)\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzevZcgmJmhi"
      },
      "source": [
        "## First steps with the gym interface\n",
        "\n",
        "As you have noticed in the previous notebooks, an environment that follows the gym interface is quite simple to use.\n",
        "It provides to this user mainly three methods:\n",
        "- `reset()` called at the beginning of an episode, it returns an observation\n",
        "- `step(action)` called to take an action with the environment, it returns the next observation, the immediate reward, whether the episode is over and additional information\n",
        "- (Optional) `render(method='human')` which allow to visualize the agent in action. Note that graphical interface does not work on google colab, so we cannot use it directly (we have to rely on `method='rbg_array'` to retrieve an image of the scene\n",
        "\n",
        "Under the hood, it also contains two useful properties:\n",
        "- `observation_space` which one of the gym spaces (`Discrete`, `Box`, ...) and describe the type and shape of the observation\n",
        "- `action_space` which is also a gym space object that describes the action space, so the type of action that can be taken\n",
        "\n",
        "The best way to learn about gym spaces is to look at the [source code](https://github.com/openai/gym/tree/master/gym/spaces), but you need to know at least the main ones:\n",
        "- `gym.spaces.Box`: A (possibly unbounded) box in $R^n$. Specifically, a Box represents the Cartesian product of n closed intervals. Each interval has the form of one of [a, b], (-oo, b], [a, oo), or (-oo, oo). Example: A 1D-Vector or an image observation can be described with the Box space.\n",
        "```python\n",
        "# Example for using image as input:\n",
        "observation_space = spaces.Box(low=0, high=255, shape=(HEIGHT, WIDTH, N_CHANNELS), dtype=np.uint8)\n",
        "```                                       \n",
        "\n",
        "- `gym.spaces.Discrete`: A discrete space in $\\{ 0, 1, \\dots, n-1 \\}$\n",
        "  Example: if you have two actions (\"left\" and \"right\") you can represent your action space using `Discrete(2)`, the first action will be 0 and the second 1.\n",
        "\n",
        "\n",
        "\n",
        "[Documentation on custom env](https://stable-baselines3.readthedocs.io/en/master/guide/custom_env.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I98IKKyNJl6K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "983c7f54-a408-4e27-8215-899e4975de84"
      },
      "source": [
        "import gym\n",
        "\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "\n",
        "# Box(4,) means that it is a Vector with 4 components\n",
        "print(\"Observation space:\", env.observation_space)\n",
        "print(\"Shape:\", env.observation_space.shape)\n",
        "# Discrete(2) means that there is two discrete actions\n",
        "print(\"Action space:\", env.action_space)\n",
        "\n",
        "# The reset method is called at the beginning of an episode\n",
        "obs = env.reset()\n",
        "# Sample a random action\n",
        "action = env.action_space.sample()\n",
        "print(\"Sampled action:\", action)\n",
        "obs, reward, done, info = env.step(action)\n",
        "# Note the obs is a numpy array\n",
        "# info is an empty dict for now but can contain any debugging info\n",
        "# reward is a scalar\n",
        "print(obs.shape, reward, done, info)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Observation space: Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32)\n",
            "Shape: (4,)\n",
            "Action space: Discrete(2)\n",
            "Sampled action: 0\n",
            "(4,) 1.0 False {}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqxatIwPOXe_"
      },
      "source": [
        "##  Gym env skeleton\n",
        "\n",
        "In practice this is how a gym environment looks like.\n",
        "Here, we have implemented a simple grid world were the agent must learn to go always left."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYzDXA9vJfz1"
      },
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "from gym import spaces\n",
        "\n",
        "\n",
        "class GoLeftEnv(gym.Env):\n",
        "  \"\"\"\n",
        "  Custom Environment that follows gym interface.\n",
        "  This is a simple env where the agent must learn to go always left. \n",
        "  \"\"\"\n",
        "  # Because of google colab, we cannot implement the GUI ('human' render mode)\n",
        "  metadata = {'render.modes': ['console']}\n",
        "  # Define constants for clearer code\n",
        "  LEFT = 0\n",
        "  RIGHT = 1\n",
        "\n",
        "  def __init__(self, grid_size=10):\n",
        "    super(GoLeftEnv, self).__init__()\n",
        "\n",
        "    # Size of the 1D-grid\n",
        "    self.grid_size = grid_size\n",
        "    # Initialize the agent at the right of the grid\n",
        "    self.agent_pos = grid_size - 1\n",
        "\n",
        "    # Define action and observation space\n",
        "    # They must be gym.spaces objects\n",
        "    # Example when using discrete actions, we have two: left and right\n",
        "    n_actions = 2\n",
        "    self.action_space = spaces.Discrete(n_actions)\n",
        "    # The observation will be the coordinate of the agent\n",
        "    # this can be described both by Discrete and Box space\n",
        "    self.observation_space = spaces.Box(low=0, high=self.grid_size,\n",
        "                                        shape=(1,), dtype=np.float32)\n",
        "\n",
        "  def reset(self):\n",
        "    \"\"\"\n",
        "    Important: the observation must be a numpy array\n",
        "    :return: (np.array) \n",
        "    \"\"\"\n",
        "    # Initialize the agent at the right of the grid\n",
        "    self.agent_pos = self.grid_size - 1\n",
        "    # here we convert to float32 to make it more general (in case we want to use continuous actions)\n",
        "    return np.array([self.agent_pos]).astype(np.float32)\n",
        "\n",
        "  def step(self, action):\n",
        "    if action == self.LEFT:\n",
        "      self.agent_pos -= 1\n",
        "    elif action == self.RIGHT:\n",
        "      self.agent_pos += 1\n",
        "    else:\n",
        "      raise ValueError(\"Received invalid action={} which is not part of the action space\".format(action))\n",
        "\n",
        "    # Account for the boundaries of the grid\n",
        "    self.agent_pos = np.clip(self.agent_pos, 0, self.grid_size)\n",
        "\n",
        "    # Are we at the left of the grid?\n",
        "    done = bool(self.agent_pos == 0)\n",
        "\n",
        "    # Null reward everywhere except when reaching the goal (left of the grid)\n",
        "    reward = 1 if self.agent_pos == 0 else 0\n",
        "\n",
        "    # Optionally we can pass additional info, we are not using that for now\n",
        "    info = {}\n",
        "\n",
        "    return np.array([self.agent_pos]).astype(np.float32), reward, done, info\n",
        "\n",
        "  def render(self, mode='console'):\n",
        "    if mode != 'console':\n",
        "      raise NotImplementedError()\n",
        "    # agent is represented as a cross, rest as a dot\n",
        "    print(\".\" * self.agent_pos, end=\"\")\n",
        "    print(\"x\", end=\"\")\n",
        "    print(\".\" * (self.grid_size - self.agent_pos))\n",
        "\n",
        "  def close(self):\n",
        "    pass\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zy5mlho1-Ine"
      },
      "source": [
        "### Validate the environment\n",
        "\n",
        "Stable Baselines3 provides a [helper](https://stable-baselines3.readthedocs.io/en/master/common/env_checker.html) to check that your environment follows the Gym interface. It also optionally checks that the environment is compatible with Stable-Baselines (and emits warning if necessary)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DOpP_B0-LXm"
      },
      "source": [
        "from stable_baselines3.common.env_checker import check_env"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CcUVatq-P0l"
      },
      "source": [
        "env = GoLeftEnv()\n",
        "# If the environment don't follow the interface, an error will be thrown\n",
        "check_env(env, warn=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJ3khFtkSE0g"
      },
      "source": [
        "### Testing the environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i62yf2LvSAYY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bc369a4-9e3f-40d7-eb62-7f3d394a54ae"
      },
      "source": [
        "env = GoLeftEnv(grid_size=10)\n",
        "\n",
        "obs = env.reset()\n",
        "env.render()\n",
        "\n",
        "print(env.observation_space)\n",
        "print(env.action_space)\n",
        "print(env.action_space.sample())\n",
        "\n",
        "GO_LEFT = 0\n",
        "# Hardcoded best agent: always go left!\n",
        "n_steps = 20\n",
        "for step in range(n_steps):\n",
        "  print(\"Step {}\".format(step + 1))\n",
        "  obs, reward, done, info = env.step(GO_LEFT)\n",
        "  print('obs=', obs, 'reward=', reward, 'done=', done)\n",
        "  env.render()\n",
        "  if done:\n",
        "    print(\"Goal reached!\", \"reward=\", reward)\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".........x.\n",
            "Box(0.0, 10.0, (1,), float32)\n",
            "Discrete(2)\n",
            "1\n",
            "Step 1\n",
            "obs= [8.] reward= 0 done= False\n",
            "........x..\n",
            "Step 2\n",
            "obs= [7.] reward= 0 done= False\n",
            ".......x...\n",
            "Step 3\n",
            "obs= [6.] reward= 0 done= False\n",
            "......x....\n",
            "Step 4\n",
            "obs= [5.] reward= 0 done= False\n",
            ".....x.....\n",
            "Step 5\n",
            "obs= [4.] reward= 0 done= False\n",
            "....x......\n",
            "Step 6\n",
            "obs= [3.] reward= 0 done= False\n",
            "...x.......\n",
            "Step 7\n",
            "obs= [2.] reward= 0 done= False\n",
            "..x........\n",
            "Step 8\n",
            "obs= [1.] reward= 0 done= False\n",
            ".x.........\n",
            "Step 9\n",
            "obs= [0.] reward= 1 done= True\n",
            "x..........\n",
            "Goal reached! reward= 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pv1e1qJETfHU"
      },
      "source": [
        "### Try it with Stable-Baselines\n",
        "\n",
        "Once your environment follow the gym interface, it is quite easy to plug in any algorithm from stable-baselines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQfLBE28SNDr"
      },
      "source": [
        "from stable_baselines3 import PPO, A2C # DQN coming soon\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "\n",
        "# Instantiate the env\n",
        "env = GoLeftEnv(grid_size=10)\n",
        "# wrap it\n",
        "env = make_vec_env(lambda: env, n_envs=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRV4Q7FVUKB6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab707b34-b501-4c7d-eea9-ac6abf6a0e73"
      },
      "source": [
        "# Train the agent\n",
        "model = A2C('MlpPolicy', env, verbose=1).learn(5000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 544      |\n",
            "|    iterations         | 100      |\n",
            "|    time_elapsed       | 0        |\n",
            "|    total_timesteps    | 500      |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.38    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 99       |\n",
            "|    policy_loss        | 6.01e-05 |\n",
            "|    value_loss         | 2.4e-07  |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 607      |\n",
            "|    iterations         | 200      |\n",
            "|    time_elapsed       | 1        |\n",
            "|    total_timesteps    | 1000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.243   |\n",
            "|    explained_variance | 5.96e-08 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 199      |\n",
            "|    policy_loss        | 4.34e-06 |\n",
            "|    value_loss         | 4.89e-09 |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 634      |\n",
            "|    iterations         | 300      |\n",
            "|    time_elapsed       | 2        |\n",
            "|    total_timesteps    | 1500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.307   |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 299      |\n",
            "|    policy_loss        | 0.00158  |\n",
            "|    value_loss         | 2.16e-05 |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 646       |\n",
            "|    iterations         | 400       |\n",
            "|    time_elapsed       | 3         |\n",
            "|    total_timesteps    | 2000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.267    |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 399       |\n",
            "|    policy_loss        | -6.05e-06 |\n",
            "|    value_loss         | 7.33e-09  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 625       |\n",
            "|    iterations         | 500       |\n",
            "|    time_elapsed       | 3         |\n",
            "|    total_timesteps    | 2500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.284    |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 499       |\n",
            "|    policy_loss        | -2.7e-05  |\n",
            "|    value_loss         | 1.21e-07  |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 563      |\n",
            "|    iterations         | 600      |\n",
            "|    time_elapsed       | 5        |\n",
            "|    total_timesteps    | 3000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.487   |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 599      |\n",
            "|    policy_loss        | 0.000105 |\n",
            "|    value_loss         | 2.99e-07 |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 520      |\n",
            "|    iterations         | 700      |\n",
            "|    time_elapsed       | 6        |\n",
            "|    total_timesteps    | 3500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.493   |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 699      |\n",
            "|    policy_loss        | 1.39e-05 |\n",
            "|    value_loss         | 5.01e-09 |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 490       |\n",
            "|    iterations         | 800       |\n",
            "|    time_elapsed       | 8         |\n",
            "|    total_timesteps    | 4000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.442    |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 799       |\n",
            "|    policy_loss        | 0.00037   |\n",
            "|    value_loss         | 5.4e-06   |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 466      |\n",
            "|    iterations         | 900      |\n",
            "|    time_elapsed       | 9        |\n",
            "|    total_timesteps    | 4500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.508   |\n",
            "|    explained_variance | 0.0667   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 899      |\n",
            "|    policy_loss        | -0.00017 |\n",
            "|    value_loss         | 1.4e-07  |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 460      |\n",
            "|    iterations         | 1000     |\n",
            "|    time_elapsed       | 10       |\n",
            "|    total_timesteps    | 5000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.671   |\n",
            "|    explained_variance | -2.73    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 999      |\n",
            "|    policy_loss        | 9.32e-05 |\n",
            "|    value_loss         | 3.76e-08 |\n",
            "------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJbeiF0RUN-p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6620a5d-5c39-4eb0-a6de-82b2cd129577"
      },
      "source": [
        "# Test the trained agent\n",
        "obs = env.reset()\n",
        "n_steps = 20\n",
        "for step in range(n_steps):\n",
        "  action, _ = model.predict(obs, deterministic=True)\n",
        "  print(\"Step {}\".format(step + 1))\n",
        "  print(\"Action: \", action)\n",
        "  obs, reward, done, info = env.step(action)\n",
        "  print('obs=', obs, 'reward=', reward, 'done=', done)\n",
        "  env.render(mode='console')\n",
        "  if done:\n",
        "    # Note that the VecEnv resets automatically\n",
        "    # when a done signal is encountered\n",
        "    print(\"Goal reached!\", \"reward=\", reward)\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1\n",
            "Action:  [1]\n",
            "obs= [[10.]] reward= [0.] done= [False]\n",
            "..........x\n",
            "Step 2\n",
            "Action:  [1]\n",
            "obs= [[10.]] reward= [0.] done= [False]\n",
            "..........x\n",
            "Step 3\n",
            "Action:  [1]\n",
            "obs= [[10.]] reward= [0.] done= [False]\n",
            "..........x\n",
            "Step 4\n",
            "Action:  [1]\n",
            "obs= [[10.]] reward= [0.] done= [False]\n",
            "..........x\n",
            "Step 5\n",
            "Action:  [1]\n",
            "obs= [[10.]] reward= [0.] done= [False]\n",
            "..........x\n",
            "Step 6\n",
            "Action:  [1]\n",
            "obs= [[10.]] reward= [0.] done= [False]\n",
            "..........x\n",
            "Step 7\n",
            "Action:  [1]\n",
            "obs= [[10.]] reward= [0.] done= [False]\n",
            "..........x\n",
            "Step 8\n",
            "Action:  [1]\n",
            "obs= [[10.]] reward= [0.] done= [False]\n",
            "..........x\n",
            "Step 9\n",
            "Action:  [1]\n",
            "obs= [[10.]] reward= [0.] done= [False]\n",
            "..........x\n",
            "Step 10\n",
            "Action:  [1]\n",
            "obs= [[10.]] reward= [0.] done= [False]\n",
            "..........x\n",
            "Step 11\n",
            "Action:  [1]\n",
            "obs= [[10.]] reward= [0.] done= [False]\n",
            "..........x\n",
            "Step 12\n",
            "Action:  [1]\n",
            "obs= [[10.]] reward= [0.] done= [False]\n",
            "..........x\n",
            "Step 13\n",
            "Action:  [1]\n",
            "obs= [[10.]] reward= [0.] done= [False]\n",
            "..........x\n",
            "Step 14\n",
            "Action:  [1]\n",
            "obs= [[10.]] reward= [0.] done= [False]\n",
            "..........x\n",
            "Step 15\n",
            "Action:  [1]\n",
            "obs= [[10.]] reward= [0.] done= [False]\n",
            "..........x\n",
            "Step 16\n",
            "Action:  [1]\n",
            "obs= [[10.]] reward= [0.] done= [False]\n",
            "..........x\n",
            "Step 17\n",
            "Action:  [1]\n",
            "obs= [[10.]] reward= [0.] done= [False]\n",
            "..........x\n",
            "Step 18\n",
            "Action:  [1]\n",
            "obs= [[10.]] reward= [0.] done= [False]\n",
            "..........x\n",
            "Step 19\n",
            "Action:  [1]\n",
            "obs= [[10.]] reward= [0.] done= [False]\n",
            "..........x\n",
            "Step 20\n",
            "Action:  [1]\n",
            "obs= [[10.]] reward= [0.] done= [False]\n",
            "..........x\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3KgI5g0ju3l"
      },
      "source": [
        "# Keras-Gym Tutorial"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FntfyMODJei7"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Configuration parameters for the whole setup\n",
        "seed = 42\n",
        "gamma = 0.99  # Discount factor for past rewards\n",
        "max_steps_per_episode = 10000\n",
        "env = gym.make(\"CartPole-v0\")  # Create the environment\n",
        "env.seed(seed)\n",
        "eps = np.finfo(np.float32).eps.item()  # Smallest number such that 1.0 + eps != 1.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TkKDyCHj086"
      },
      "source": [
        "num_inputs = 4\n",
        "num_actions = 2\n",
        "num_hidden = 128\n",
        "\n",
        "inputs = layers.Input(shape=(num_inputs,))\n",
        "common = layers.Dense(num_hidden, activation=\"relu\")(inputs)\n",
        "action = layers.Dense(num_actions, activation=\"softmax\")(common)\n",
        "critic = layers.Dense(1)(common)\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=[action, critic])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xiyfIEYBkBr7",
        "outputId": "9ff98aa9-a5b2-4bb0-d161-9bd4257624b6"
      },
      "source": [
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 4)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 128)          640         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 2)            258         dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 1)            129         dense[0][0]                      \n",
            "==================================================================================================\n",
            "Total params: 1,027\n",
            "Trainable params: 1,027\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hhghd17DkOJK",
        "outputId": "163279b4-b7fb-466f-a528-c21b491b1092"
      },
      "source": [
        "optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
        "huber_loss = keras.losses.Huber()\n",
        "action_probs_history = []\n",
        "critic_value_history = []\n",
        "rewards_history = []\n",
        "running_reward = 0\n",
        "episode_count = 0\n",
        "\n",
        "while True:  # Run until solved\n",
        "    state = env.reset()\n",
        "    episode_reward = 0\n",
        "    with tf.GradientTape() as tape:\n",
        "        for timestep in range(1, max_steps_per_episode):\n",
        "            # env.render(); Adding this line would show the attempts\n",
        "            # of the agent in a pop up window.\n",
        "\n",
        "            state = tf.convert_to_tensor(state)\n",
        "            state = tf.expand_dims(state, 0)\n",
        "\n",
        "            # Predict action probabilities and estimated future rewards\n",
        "            # from environment state\n",
        "            action_probs, critic_value = model(state)\n",
        "            critic_value_history.append(critic_value[0, 0])\n",
        "\n",
        "            # Sample action from action probability distribution\n",
        "            action = np.random.choice(num_actions, p=np.squeeze(action_probs))\n",
        "            action_probs_history.append(tf.math.log(action_probs[0, action]))\n",
        "\n",
        "            # Apply the sampled action in our environment\n",
        "            state, reward, done, _ = env.step(action)\n",
        "            rewards_history.append(reward)\n",
        "            episode_reward += reward\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        # Update running reward to check condition for solving\n",
        "        running_reward = 0.05 * episode_reward + (1 - 0.05) * running_reward\n",
        "\n",
        "        # Calculate expected value from rewards\n",
        "        # - At each timestep what was the total reward received after that timestep\n",
        "        # - Rewards in the past are discounted by multiplying them with gamma\n",
        "        # - These are the labels for our critic\n",
        "        returns = []\n",
        "        discounted_sum = 0\n",
        "        for r in rewards_history[::-1]:\n",
        "            discounted_sum = r + gamma * discounted_sum\n",
        "            returns.insert(0, discounted_sum)\n",
        "\n",
        "        # Normalize\n",
        "        returns = np.array(returns)\n",
        "        returns = (returns - np.mean(returns)) / (np.std(returns) + eps)\n",
        "        returns = returns.tolist()\n",
        "\n",
        "        # Calculating loss values to update our network\n",
        "        history = zip(action_probs_history, critic_value_history, returns)\n",
        "        actor_losses = []\n",
        "        critic_losses = []\n",
        "        for log_prob, value, ret in history:\n",
        "            # At this point in history, the critic estimated that we would get a\n",
        "            # total reward = `value` in the future. We took an action with log probability\n",
        "            # of `log_prob` and ended up recieving a total reward = `ret`.\n",
        "            # The actor must be updated so that it predicts an action that leads to\n",
        "            # high rewards (compared to critic's estimate) with high probability.\n",
        "            diff = ret - value\n",
        "            actor_losses.append(-log_prob * diff)  # actor loss\n",
        "\n",
        "            # The critic must be updated so that it predicts a better estimate of\n",
        "            # the future rewards.\n",
        "            critic_losses.append(\n",
        "                huber_loss(tf.expand_dims(value, 0), tf.expand_dims(ret, 0))\n",
        "            )\n",
        "\n",
        "        # Backpropagation\n",
        "        loss_value = sum(actor_losses) + sum(critic_losses)\n",
        "        grads = tape.gradient(loss_value, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "        # Clear the loss and reward history\n",
        "        action_probs_history.clear()\n",
        "        critic_value_history.clear()\n",
        "        rewards_history.clear()\n",
        "\n",
        "    # Log details\n",
        "    episode_count += 1\n",
        "    if episode_count % 10 == 0:\n",
        "        template = \"running reward: {:.2f} at episode {}\"\n",
        "        print(template.format(running_reward, episode_count))\n",
        "\n",
        "    if running_reward > 195:  # Condition to consider the task solved\n",
        "        print(\"Solved at episode {}!\".format(episode_count))\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "running reward: 8.93 at episode 10\n",
            "running reward: 20.89 at episode 20\n",
            "running reward: 30.46 at episode 30\n",
            "running reward: 30.69 at episode 40\n",
            "running reward: 26.42 at episode 50\n",
            "running reward: 21.04 at episode 60\n",
            "running reward: 23.64 at episode 70\n",
            "running reward: 35.10 at episode 80\n",
            "running reward: 38.89 at episode 90\n",
            "running reward: 30.15 at episode 100\n",
            "running reward: 37.57 at episode 110\n",
            "running reward: 37.56 at episode 120\n",
            "running reward: 35.97 at episode 130\n",
            "running reward: 44.34 at episode 140\n",
            "running reward: 47.83 at episode 150\n",
            "running reward: 71.22 at episode 160\n",
            "running reward: 112.90 at episode 170\n",
            "running reward: 138.88 at episode 180\n",
            "running reward: 159.28 at episode 190\n",
            "running reward: 152.98 at episode 200\n",
            "running reward: 139.37 at episode 210\n",
            "running reward: 131.02 at episode 220\n",
            "running reward: 100.06 at episode 230\n",
            "running reward: 87.68 at episode 240\n",
            "running reward: 103.92 at episode 250\n",
            "running reward: 138.12 at episode 260\n",
            "running reward: 162.95 at episode 270\n",
            "running reward: 177.82 at episode 280\n",
            "running reward: 186.72 at episode 290\n",
            "running reward: 185.24 at episode 300\n",
            "running reward: 169.30 at episode 310\n",
            "running reward: 161.65 at episode 320\n",
            "running reward: 161.87 at episode 330\n",
            "running reward: 171.18 at episode 340\n",
            "running reward: 176.20 at episode 350\n",
            "running reward: 185.75 at episode 360\n",
            "running reward: 191.47 at episode 370\n",
            "running reward: 194.89 at episode 380\n",
            "Solved at episode 381!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyXQ_qIdktmp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}